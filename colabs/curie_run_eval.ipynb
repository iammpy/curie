{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXI7uAyl7cH1"
      },
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VchTG3815jUG"
      },
      "outputs": [],
      "source": [
        "import json5\n",
        "import re\n",
        "import functools\n",
        "from typing import Any, Tuple, Union, Dict\n",
        "# from Bio import Align\n",
        "import glob\n",
        "# from bert_score import score\n",
        "# from rouge_score import rouge_scorer\n",
        "import numpy as np\n",
        "import ast\n",
        "import os\n",
        "import sys\n",
        "import json_repair\n",
        "# import google.generativeai as genai\n",
        "# import Levenshtein\n",
        "# MODEL_NAME = \"deepseek-r1\"\n",
        "# \"doubao-1.5-thinking-pro\"\n",
        "# EVAL_MODEL_NAME = \"deepseek-r1\"   # eval by openai only\n",
        "\n",
        "TASK_NAME = \"MPVE\"\n",
        "# \"DFT\"\n",
        "#  \"MPVE\"\n",
        "# \"deepseek-r1\"\n",
        "INFER_MODEL_NAME = \"deepseek-r1\"\n",
        "# # \"DeepSeek-R1-Distill-Qwen-7B\"\n",
        "# # \"DeepSeek-R1-Distill-Qwen-32B\"\n",
        "# # MODEL_NAME = \"deepseek-r1\"\n",
        "# # \"doubao-1.5-thinking-pro\"\n",
        "\n",
        "model_list=[\n",
        "    \"deepseek-r1\",\n",
        "    \"doubao-1.5-thinking-pro\",\n",
        "    \"DeepSeek-R1-Distill-Qwen-7B\",\n",
        "    \"DeepSeek-R1-Distill-Qwen-32B\",\n",
        "    \n",
        "]\n",
        "\n",
        "# --- 从命令行参数获取模型名称 ---\n",
        "if \"__file__\" in globals() :\n",
        "    # 表示当前是一个py文件\n",
        "    if len(sys.argv) > 1:\n",
        "        INFER_MODEL_NAME = sys.argv[1]\n",
        "        print(f\"从命令行参数获取模型名称: {INFER_MODEL_NAME}\")\n",
        "    else:\n",
        "        # 如果没有提供命令行参数，使用默认值或提示错误\n",
        "        INFER_MODEL_NAME = \"DeepSeek-R1-Distill-Qwen-32B\" # 默认值\n",
        "        print(f\"警告: 未在命令行提供模型名称，将使用默认值: {INFER_MODEL_NAME}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if \"__file__\" in globals() :\n",
        "    # 表示当前是一个py文件\n",
        "    os.chdir(os.path.dirname(__file__))\n",
        "    print(\"当前路径:\", os.getcwd())\n",
        "    print(os.path.abspath('./'))\n",
        "file_root_path=\"..\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRmqK0PU7Pv9"
      },
      "source": [
        "# eval functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 服务器模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model import *\n",
        "# call_server(messages=\"你是谁\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 读取配置文件\n",
        "if \"__file__\" in globals() :\n",
        "    # 表示当前是一个py文件\n",
        "    config_file = os.path.join(os.path.dirname(__file__), \"api_config.yaml\")\n",
        "else:\n",
        "    # 表示当前是一个交互式环境\n",
        "    config_file = os.path.join(os.getcwd(), \"api_config.yaml\")\n",
        "# config_file = \"api_config.yaml\"\n",
        "api_config = yaml.safe_load(open(config_file, \"r\", encoding=\"utf-8\"))\n",
        "model_name = \"gpt-4o-2024-11-20\"\n",
        "model_cfg = api_config.get(model_name)\n",
        "from openai import OpenAI\n",
        "openai_client = OpenAI(\n",
        "        api_key= api_config.get(model_name, {}).get(\"api_key\"),\n",
        "        base_url= api_config.get(model_name, {}).get(\"base_url\"),\n",
        "        max_retries= api_config.get(model_name, {}).get(\"max_retries\"),\n",
        "        timeout= api_config.get(model_name, {}).get(\"timeout\"),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5-94GHgHW4C"
      },
      "source": [
        "### LLMSim util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N10Sp_IDHal6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def eval_overall_result(\n",
        "    eval_output_item: dict[str, Any], verbose: bool = False\n",
        ") -> dict[str, Any]:\n",
        "  \"\"\"Gets overall eval result.\n",
        "\n",
        "  Args:\n",
        "    eval_output_item: eval output item.\n",
        "    verbose: whether to print model eval original output.\n",
        "\n",
        "  Returns:\n",
        "    overall eval result.\n",
        "  \"\"\"\n",
        "  num_match = sum([\n",
        "      1 if (\"json_extracted_index\" in item) else 0\n",
        "      for item in eval_output_item[\"response_json\"]\n",
        "  ])\n",
        "  if verbose:\n",
        "    print(\"Model eval original output:\\n\", eval_output_item)\n",
        "  num_gt = eval_output_item[\"ground_truth_length\"]\n",
        "  num_response = eval_output_item[\"model_response_length\"]\n",
        "  pre = min(num_match / num_response if num_response else 0, 1.0)\n",
        "  rec = min(num_match / num_gt if num_gt else 0, 1.0)\n",
        "  return {\n",
        "      \"num_match\": num_match,\n",
        "      \"num_ground_truth\": num_gt,\n",
        "      \"num_model_response\": num_response,\n",
        "      \"precision\": pre,\n",
        "      \"recall\": rec,\n",
        "      \"f1\": 2.0 * pre * rec / (pre + rec) if pre + rec else 0.0,\n",
        "  }\n",
        "\n",
        "# def llm_output(client: Any, prompt: str) -> str:\n",
        "#   # client=None for external api\n",
        "#   return get_model().generate_content(prompt).text\n",
        "def load_matsci_prompt(filepath: str) -> str:\n",
        "  \"\"\"Loads matsci prompt.\n",
        "\n",
        "  Args:\n",
        "    filepath: filepath of prompt.\n",
        "\n",
        "  Returns:\n",
        "    Loaded prompt.\n",
        "  \"\"\"\n",
        "  # return resources.GetResource(filepath).decode(\"utf-8\").strip()\n",
        "  with open(filepath, 'r') as file:\n",
        "    text_content = file.read()\n",
        "  return text_content.strip()\n",
        "\n",
        "\n",
        "def model_eval_json(\n",
        "    record_id: str | None,\n",
        "    json_ground_truth: list[dict[str, Any]],\n",
        "    json_model_response: list[dict[str, Any]],\n",
        "    eval_prompt: str,\n",
        "    client: Any,\n",
        ") -> dict[str, Any]:\n",
        "  \"\"\"Evaluate with json ground truth and model response.\"\"\"\n",
        "  eval_list = []\n",
        "  \n",
        "  \n",
        "  for k, model_response_item in enumerate(json_model_response):\n",
        "      # Add index to llm input to suppress hallucination on json_extracted_index\n",
        "      # prediction\n",
        "      try:\n",
        "        model_response_item[\"json_extracted_index\"] = k\n",
        "      except Exception as e:\n",
        "        print(\"Error in loading prompt: \", e)\n",
        "        print(\"record_id: \", record_id)\n",
        "        # print(\"json_ground_truth: \", json_ground_truth)\n",
        "        print(\"model_response_item: \", model_response_item)\n",
        "        \n",
        "  for j, ground_truth_item in enumerate(json_ground_truth):\n",
        "    prompt = (\n",
        "        load_matsci_prompt(eval_prompt)\n",
        "        .replace(\n",
        "            \"{{json_ground_truth}}\", json5.dumps(ground_truth_item, indent=2)\n",
        "        )\n",
        "        .replace(\n",
        "            \"{{json_extracted_list}}\", json5.dumps(json_model_response, indent=2)\n",
        "        )\n",
        "    )\n",
        "    _ , output = call_openai(\n",
        "      messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "      client=openai_client\n",
        "    )\n",
        "\n",
        "    try:\n",
        "      output_json = json_repair.repair_json(output,return_objects=True)\n",
        "    except Exception as e:  # pylint: disable=broad-except\n",
        "      print(\"Skipping incomplete last item in output: \", e)\n",
        "      inds = [m.start() for m in re.finditer(r\",\\s*\\{\", output)]\n",
        "      if inds:\n",
        "        ind = inds[-1]\n",
        "        output_json = json5.loads(output[:ind] + \"]\")\n",
        "      else:\n",
        "        output_json = []\n",
        "    if isinstance(output_json, list):\n",
        "      # Handle edge case that model hallucinated outputing list enclosing json.\n",
        "      if not output_json:\n",
        "        output_json = {}\n",
        "      else:\n",
        "        output_json = output_json[0]\n",
        "    output_json[\"json_ground_truth_index\"] = j\n",
        "    output_json[\"json_ground_truth\"] = ground_truth_item\n",
        "    output_json[\"json_extracted\"] = {}\n",
        "    # If not in it, it means llm didn't find a good match, so leave it empty.\n",
        "    if \"json_extracted_index\" in output_json:\n",
        "      if (\n",
        "          str(output_json[\"json_extracted_index\"]).isdigit()\n",
        "          and int(output_json[\"json_extracted_index\"]) > 0\n",
        "          and int(output_json[\"json_extracted_index\"])\n",
        "          < len(json_model_response)\n",
        "      ):\n",
        "        output_json[\"json_extracted\"] = json_model_response[\n",
        "            int(output_json[\"json_extracted_index\"])\n",
        "        ]\n",
        "      else:\n",
        "        del output_json[\"json_extracted_index\"]\n",
        "    eval_list.append(output_json)\n",
        "  return {\n",
        "      \"record_id\": record_id,\n",
        "      \"ground_truth_length\": len(json_ground_truth),\n",
        "      \"model_response_length\": len(json_model_response),\n",
        "      \"response_json\": eval_list,\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_prompt(*args)->list[dict]:\n",
        "    # 将输入的一系列字符串转化为一个字典列表\n",
        "    # 如果没有导入json5模块，就导入json5\n",
        "    # import json5\n",
        "    list_dict = []\n",
        "    for i,arg in enumerate(args):\n",
        "        if i%2==0:\n",
        "            # 偶数索引为用户输入\n",
        "            temp={\n",
        "                \"role\": \"user\",\n",
        "                \"content\": arg\n",
        "            }\n",
        "        else:\n",
        "            temp={\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": arg\n",
        "            }\n",
        "        list_dict.append(temp)\n",
        "    \n",
        "    return list_dict\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3q-IXimHghk"
      },
      "source": [
        "### LLMSim dft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cZpvEbZMHpD8"
      },
      "outputs": [],
      "source": [
        "def get_lmsim_score_dft(prediction, reference,paper_id):\n",
        "  return dft_domain_expert_model_based_eval(reference, prediction) # Client=None for external api\n",
        "\n",
        "\n",
        "_METADATA_EVAL_PROMPT_FILENAME = file_root_path + \"/prompts/dft_metadata_eval_output_1_shot.txt\"\n",
        "_STRUCTURE_EVAL_PROMPT_FILENAME = file_root_path + \"/prompts/dft_structure_eval_output_1_shot.txt\"\n",
        "\n",
        "\n",
        "def get_dft_model_response_field(\n",
        "    model_output_value: dict[str, Any], field_name: str\n",
        ") -> list[Any] | str:\n",
        "  \"\"\"Returns the model response for a given field from the inference output.\n",
        "\n",
        "     This applies to json responses from the dft chained inference output.\n",
        "\n",
        "  Args:\n",
        "    model_output_value: The model output response (from one two) as a dict.\n",
        "    field_name: The name of the field to extract. This should be one of\n",
        "      \"structure_metadata\", \"dft_metadata\", or \"code\".\n",
        "  \"\"\"\n",
        "  if field_name in [\"structure_metadata\"]:\n",
        "    if field_name in model_output_value:\n",
        "      print(model_output_value)\n",
        "      return model_output_value[\"structure_metadata\"]\n",
        "    else:\n",
        "      return \"\"\n",
        "\n",
        "  elif field_name in [\"dft_metadata\"]:\n",
        "    if field_name in model_output_value:\n",
        "      return model_output_value[\"dft_metadata\"]\n",
        "    else:\n",
        "      return \"\"\n",
        "\n",
        "  elif field_name == \"code\":\n",
        "    if field_name in model_output_value:\n",
        "      code = \"\\n\".join(\n",
        "          [x[\"code_element\"] for x in model_output_value[\"code_elements\"]]\n",
        "      )\n",
        "      code += \"\\n\" + model_output_value[\"execution_code\"]\n",
        "      return code\n",
        "    else:\n",
        "      return \"\"\n",
        "\n",
        "  raise ValueError(f\"Unknown field name: {field_name}\")\n",
        "\n",
        "\n",
        "def get_annotated_structure_metadata_and_dft_params(\n",
        "    gt_paper_code: str, verbose: int = 0\n",
        ") -> dict[str, list[str]]:\n",
        "  \"\"\"Returns structure metadata and dft params from the ground truth code.\n",
        "\n",
        "  Args:\n",
        "    gt_paper_code: The ground truth code from .py file as a string.\n",
        "    verbose: The verbosity level.\n",
        "  \"\"\"\n",
        "  structures = []\n",
        "  dft_params = []\n",
        "\n",
        "  paper = gt_paper_code\n",
        "  if \"structure_metadata_\" in paper:\n",
        "    parts = paper.split(\"structure_metadata_\")[1:]\n",
        "    whole_parts = [\"structure_metadata_\" + part for part in parts]\n",
        "\n",
        "    for part in whole_parts:\n",
        "      if verbose > 1:\n",
        "        print(\"PART:\\n\", part)\n",
        "      if \"parse_raw(\" not in part:\n",
        "        continue\n",
        "      left, right, *_ = part.split(\"parse_raw(\")\n",
        "      if \"StructureMetadata\" in left:\n",
        "        end_struc = \")\"\n",
        "        if \"')\" in right:\n",
        "          end_struc = \"')\"\n",
        "        elif \"'\\n)\" in right:\n",
        "          end_struc = \"'\\n)\"\n",
        "        struc_json = right.split(end_struc)[0].strip()\n",
        "        if verbose > 0:\n",
        "          print(\"Extracted structure:\\n\", struc_json)\n",
        "        # clean_json = struc_json.replace('NaN', '\"NaN\"')\n",
        "        # struc_json = ast.literal_eval(clean_json)\n",
        "        structures.append(struc_json)\n",
        "\n",
        "  if \"dft_params_\" in paper:\n",
        "    parts = paper.split(\"dft_params_\")[1:]\n",
        "    # print(parts)\n",
        "    whole_parts = [\"dft_params_\" + part for part in parts]\n",
        "\n",
        "    for part in whole_parts:\n",
        "      if verbose > 1:\n",
        "        print(\"PART:\\n\", part)\n",
        "      if \"parse_raw(\" not in part:\n",
        "        continue\n",
        "      left, right, *_ = part.split(\"parse_raw(\")\n",
        "      if \"DFTParameters\" in left:\n",
        "        end_struc = \")\"\n",
        "        if \"')\" in right:\n",
        "          end_struc = \"')\"\n",
        "        elif \"'\\n)\" in right:\n",
        "          end_struc = \"'\\n)\"\n",
        "        dft_params_str = right.split(end_struc)[0].strip()\n",
        "        if verbose > 0:\n",
        "          print(\"Extracted dft_param:\\n\", dft_params_str)\n",
        "        # clean_json = dft_params_str.replace('NaN', '\"NaN\"')\n",
        "        # dft_params_str = ast.literal_eval(clean_json)\n",
        "        dft_params.append(dft_params_str)\n",
        "\n",
        "  gt_struc_jsons = []\n",
        "  for struct_metadata in structures:\n",
        "    gt_json = struct_metadata.split(\"'\")[1]\n",
        "    clean_json = gt_json.replace(\"NaN\", '\"NaN\"')\n",
        "    try:\n",
        "      gt_structure_json = ast.literal_eval(clean_json)\n",
        "      gt_struc_jsons.append(gt_structure_json)\n",
        "    except Exception:  # pylint: disable=broad-exception-caught\n",
        "      gt_struc_jsons.append(clean_json)\n",
        "\n",
        "  gt_dft_params_jsons = []\n",
        "  for dft_param in dft_params:\n",
        "    gt_json = dft_param.split(\"'\")[1]\n",
        "    clean_json = gt_json.replace(\"NaN\", '\"NaN\"')\n",
        "    try:\n",
        "      gt_dft_json = ast.literal_eval(clean_json)\n",
        "      gt_dft_params_jsons.append(gt_dft_json)\n",
        "    except Exception:  # pylint: disable=broad-exception-caught\n",
        "      gt_dft_params_jsons.append(clean_json)\n",
        "\n",
        "  return {\n",
        "      \"structures_metadata\": gt_struc_jsons,\n",
        "      \"dft_params\": gt_dft_params_jsons,\n",
        "  }\n",
        "\n",
        "\n",
        "def get_material_composition_from_struc(\n",
        "    structure: str | dict[str, str],\n",
        ") -> str | None:\n",
        "  \"\"\"Returns material composition from the structure metadata dict or string.\"\"\"\n",
        "  if isinstance(structure, dict):\n",
        "    if \"composition\" in structure:\n",
        "      return structure[\"composition\"]\n",
        "  elif isinstance(structure, str):\n",
        "    if \"composition\" in structure:\n",
        "      material = structure.split(r\"\\\"composition\\\":\")[1].split(\",\")[0]\n",
        "      return material\n",
        "  return None\n",
        "\n",
        "\n",
        "def get_json_from_str(input_str: str) -> dict[str, Any] | None:\n",
        "  \"\"\"Returns the json object from the ground truth input string.\"\"\"\n",
        "  output_val = input_str.replace(\"NaN\", '\"NaN\"')\n",
        "  output_val = output_val.replace(\"true\", '\"1.0\"')\n",
        "  output_val = output_val.replace(\"false\", '\"NaN\"')\n",
        "  try:\n",
        "    output_val = ast.literal_eval(output_val)\n",
        "  except ValueError:\n",
        "    return None\n",
        "  return output_val\n",
        "\n",
        "\n",
        "def parse_ground_truth_dft(ground_truth: str, client: Any) -> list[dict[str, Any]]:\n",
        "  \"\"\"Parses ground truth.\"\"\"\n",
        "  try:\n",
        "    json_ground_truth = json5.loads(\n",
        "        ground_truth.replace(\"\\n\", \"\").replace(\"\\\\\", \"\")\n",
        "    )\n",
        "    if json_ground_truth and isinstance(json_ground_truth[0], str):\n",
        "      json_ground_truth = [json5.loads(item) for item in json_ground_truth]\n",
        "  except Exception:  # pylint: disable=broad-except\n",
        "    print(\"***using llm to parse\")\n",
        "    _ , ground_truth = call_openai(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Extract ground truth json list from the following text.\\n\"\n",
        "                + ground_truth\n",
        "                + \"\\nMake sure to remove all backslashes for escape characters. Output\"\n",
        "                \" the json list ONLY, without any explanation, prefix or suffix:\\n\",\n",
        "            }\n",
        "        ],\n",
        "        client=openai_client,\n",
        "       \n",
        "    ) \n",
        "    # print(\"***llm_ground_truth:\\n\", ground_truth)\n",
        "    json_ground_truth = json5.loads(\n",
        "        ground_truth.replace(\"\\n\", \"\").replace('\\\\\"', \"\").replace(\"\\\\\", \"\")\n",
        "    )\n",
        "\n",
        "  return json_ground_truth\n",
        "\n",
        "\n",
        "def parse_model_response_dft(\n",
        "    model_response: str, client: Any, use_llm=False\n",
        ") -> list[dict[str, Any]]:\n",
        "  \"\"\"Parses model response.\"\"\"\n",
        "  def remove_prefix_suffix(text):\n",
        "    return text.replace(\"\\n\", \"\").removeprefix(\"```json\").removesuffix(\"```json\").removeprefix(\"```\").removesuffix(\"```\").removeprefix(\"`\").removesuffix(\"`\")\n",
        "  if use_llm:\n",
        "    _,model_response = call_openai(\n",
        "          messages=[\n",
        "              {\n",
        "                  \"role\": \"user\",\n",
        "                  \"content\": \"Extract json list from the following text.\\n\"\n",
        "                  + model_response\n",
        "                  + \"\\nMake sure to remove all backslashes for escape characters. Output\"\n",
        "                  \" the json list ONLY, without any explanation, prefix or suffix:\\n\",\n",
        "              }\n",
        "          ],\n",
        "          client=openai_client,\n",
        "          )\n",
        "    \n",
        "  response_text = remove_prefix_suffix(model_response)\n",
        "  # response_text = model_response\n",
        "  # try:\n",
        "  #   try:\n",
        "  #     formatted_text = re.sub(r\"(?<=\\w)'(?=\\w|\\s)\", \"\\\\'\", response_text)\n",
        "  #     if not formatted_text:\n",
        "  #       formatted_text = \"{}\"\n",
        "  #       print(\"Response_text is empty\")\n",
        "  #     json_model_response = json5.load(formatted_text)\n",
        "  #   except Exception as e:  # pylint: disable=broad-except\n",
        "  #     print(\"Skipping incomplete last item: \", e)\n",
        "  #     # print(\"***\", response_text)\n",
        "  #     ind = [m.start() for m in re.finditer(r\",\\s*\\{\", response_text)][-1]\n",
        "  #     json_model_response = json5.loads(response_text[:ind] + \"]\")\n",
        "  # except:\n",
        "  #     return parse_model_response_dft(model_response, client, use_llm=True) if not use_llm else []\n",
        "  # return json_model_response\n",
        "\n",
        "  try:\n",
        "    formatted_text = re.sub(r\"(?<=\\w)'(?=\\w|\\s)\", \"\\\\'\", response_text)\n",
        "    if not formatted_text:\n",
        "      formatted_text = \"{}\"\n",
        "      print(\"Response_text is empty\")\n",
        "    json_model_response = json5.loads(formatted_text)\n",
        "  except Exception as e:  # pylint: disable=broad-except\n",
        "    print(\"using llm to parse model output because: \", e)\n",
        "    return parse_model_response_dft(model_response, client, use_llm=True) if not use_llm else []\n",
        "  return json_model_response\n",
        "\n",
        "def dft_model_eval_paper(\n",
        "    record_id: str | None,\n",
        "    ground_truth: str,\n",
        "    model_response: str,\n",
        "    eval_prompt: str,\n",
        "    client: Any,\n",
        ") -> dict[str, Any]:\n",
        "  \"\"\"Runs model evaluation on material properties for a single paper.\n",
        "\n",
        "  Args:\n",
        "    record_id: record id or paper id.\n",
        "    ground_truth: ground truth list in str type.\n",
        "    model_response: model response list in str type.\n",
        "    eval_prompt: eval prompt.\n",
        "    client: llm client.\n",
        "\n",
        "  Returns:\n",
        "    model eval response json.\n",
        "  \"\"\"\n",
        "  json_ground_truth = parse_ground_truth_dft(ground_truth, client)\n",
        "  json_model_response = parse_model_response_dft(model_response, client)\n",
        "  return model_eval_json(\n",
        "      record_id=record_id,\n",
        "      json_ground_truth=json_ground_truth,\n",
        "      json_model_response=json_model_response,\n",
        "      eval_prompt=eval_prompt,\n",
        "      client=client,\n",
        "  )\n",
        "\n",
        "\n",
        "def dft_metadata_domain_expert_model_based_eval(\n",
        "    ground_truth: str,\n",
        "    model_response: str,\n",
        "    client: Any | None = None,\n",
        "    eval_prompt: str = _METADATA_EVAL_PROMPT_FILENAME,\n",
        "    verbose: bool = True,\n",
        ") -> dict[str, Any]:\n",
        "  return dft_domain_expert_model_based_eval(\n",
        "      ground_truth=ground_truth,\n",
        "      model_response=model_response,\n",
        "      client=client,\n",
        "      eval_prompt=eval_prompt,\n",
        "      verbose=verbose,\n",
        "  )\n",
        "\n",
        "\n",
        "def dft_structure_domain_expert_model_based_eval(\n",
        "    ground_truth: str,\n",
        "    model_response: str,\n",
        "    client: Any | None = None,\n",
        "    eval_prompt: str = _STRUCTURE_EVAL_PROMPT_FILENAME,\n",
        "    verbose: bool = True,\n",
        ") -> dict[str, Any]:\n",
        "  return dft_domain_expert_model_based_eval(\n",
        "      ground_truth=ground_truth,\n",
        "      model_response=model_response,\n",
        "      client=client,\n",
        "      eval_prompt=eval_prompt,\n",
        "      verbose=verbose,\n",
        "  )\n",
        "\n",
        "\n",
        "def dft_domain_expert_model_based_eval(\n",
        "    ground_truth: str,\n",
        "    model_response: str,\n",
        "    client: Any | None = None,\n",
        "    eval_prompt: str = _METADATA_EVAL_PROMPT_FILENAME,\n",
        "    verbose: bool = True,\n",
        ") -> dict[str, Any]:\n",
        "  \"\"\"Runs model based eval on dft.\n",
        "\n",
        "  Args:\n",
        "    ground_truth: ground truth list in str type.\n",
        "    model_response: model response list in str type.\n",
        "    client: llm client.\n",
        "    eval_prompt: eval prompt.\n",
        "    verbose: whether to print out eval results.\n",
        "\n",
        "  Returns:\n",
        "    eval result.\n",
        "  \"\"\"\n",
        "  if verbose:\n",
        "    print(\"Model eval started...\")\n",
        "  eval_output_item = dft_model_eval_paper(\n",
        "      record_id=None,\n",
        "      ground_truth=ground_truth,\n",
        "      model_response=model_response,\n",
        "      eval_prompt=eval_prompt,\n",
        "      client=client,\n",
        "  )\n",
        "  if verbose:\n",
        "    print(\"Model eval finished.\")\n",
        "  eval_result = eval_overall_result(\n",
        "      eval_output_item, verbose=verbose\n",
        "  )\n",
        "  if verbose:\n",
        "    print(\"Eval results:\\n\", eval_result)\n",
        "  return eval_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pukWSU7WHwQo"
      },
      "source": [
        "### LLMSim mpve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z6ufJ9PGHxo0"
      },
      "outputs": [],
      "source": [
        "def get_lmsim_score_mpve(prediction, reference,\n",
        "                         paper_id: str = \"\",\n",
        "                         ):\n",
        "  # TODO: Convert to external model client\n",
        "  return mpve_domain_expert_model_based_eval(reference, prediction,client=None,paper_id=paper_id) # Client=None for external api\n",
        "\n",
        "\n",
        "# TODO: Convert to Drive path\n",
        "_EVAL_PROMPT_FILENAME = file_root_path + \"/prompts/mat_eval_output_1_shot.txt\"\n",
        "\n",
        "\n",
        "def parse_ground_truth_mpve(ground_truth: str) -> list[dict[str, Any]]:\n",
        "  json_ground_truth = json_repair.repair_json(ground_truth.replace(\"\\n\", \"\"),return_objects=True)\n",
        "  json_ground_truth.sort(key=lambda x: x[\"index\"])\n",
        "  # remove unnecessary fields for model eval to prevent hallucination\n",
        "  for item in json_ground_truth:\n",
        "    if \"index\" in item:\n",
        "      del item[\"index\"]\n",
        "    if \"paper_id\" in item:\n",
        "      del item[\"paper_id\"]\n",
        "    if \"synonyms\" in item:\n",
        "      del item[\"synonyms\"]\n",
        "  return json_ground_truth\n",
        "\n",
        "\n",
        "def parse_model_response_mpve(model_response: str) -> list[dict[str, Any]]:\n",
        "  \"\"\"Parses model response.\n",
        "\n",
        "  Args:\n",
        "    model_response:\n",
        "\n",
        "  Returns:\n",
        "  \"\"\"\n",
        "  response_text = (\n",
        "      model_response.replace(\"\\n\", \"\")\n",
        "      .removeprefix(\" \")\n",
        "      .removesuffix(\" \")\n",
        "      .removeprefix(\"```json\")\n",
        "      .removesuffix(\"```json\")\n",
        "      .removeprefix(\"```\")\n",
        "      .removesuffix(\"```\")\n",
        "      .removeprefix(\"`\")\n",
        "      .removesuffix(\"`\")\n",
        "  )\n",
        "  try:\n",
        "    formatted_text = re.sub(r\"(?<=\\w)'(?=\\w|\\s)\", \"\\\\'\", response_text)\n",
        "    if not formatted_text:\n",
        "      formatted_text = \"{}\"\n",
        "      print(\"Response_text is empty\")\n",
        "    json_model_response = json5.load(formatted_text)\n",
        "  except Exception as e:  # pylint: disable=broad-except\n",
        "    # print(\"Skipping incomplete last item: \", e)\n",
        "    # ind = [m.start() for m in re.finditer(r\",\\s*\\{\", response_text)][-1]\n",
        "    # json_model_response = json_repair.repair_json(response_text,return_objects=True)\n",
        "    print(\"using llm to parse\")\n",
        "    _ , response_text = call_openai(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Extract model_response json list from the following text.\\n\"\n",
        "                + response_text\n",
        "                + \"\\nMake sure to remove all backslashes for escape characters. Output\"\n",
        "                \" the json list ONLY, without any explanation, prefix or suffix:\\n\",\n",
        "            }\n",
        "        ],\n",
        "        client=openai_client,\n",
        "       \n",
        "    )\n",
        "    # print(\"llm_model_response:\\n\", response_text)\n",
        "    json_model_response = json_repair.repair_json(\n",
        "        response_text.replace(\"\\n\", \"\").replace('\\\\\"', \"\").replace(\"\\\\\", \"\"),\n",
        "        return_objects=True,\n",
        "    )\n",
        "    if isinstance(json_model_response, dict):\n",
        "      json_model_response = [json_model_response]\n",
        "  return json_model_response\n",
        "  \n",
        "  if not response_text.strip():\n",
        "    print(\"Warning (parse_model_response_mpve): Model response text is empty after stripping. Returning empty list.\")\n",
        "    return []\n",
        "\n",
        "  parsed_object: Any = None\n",
        "  try:\n",
        "    # Attempt to repair and parse the JSON.\n",
        "    formatted_text = re.sub(r\"(?<=\\w)'(?=\\w|\\s)\", \"\\\\'\", response_text) \n",
        "    if not formatted_text.strip():\n",
        "        print(\"Warning (parse_model_response_mpve): Model response text became empty after formatting. Returning empty list.\")\n",
        "        return []\n",
        "    parsed_object = json_repair.repair_json(formatted_text, return_objects=True)\n",
        "  except Exception as e_repair:\n",
        "    print(f\"Warning (parse_model_response_mpve): json_repair.repair_json failed: {e_repair}. Trying fallback with json5.loads.\")\n",
        "    try:\n",
        "      # Fallback: try to load with json5 directly\n",
        "      parsed_object = json5.loads(response_text)\n",
        "    except Exception as e_json5:\n",
        "      print(f\"Warning (parse_model_response_mpve): Fallback json5.loads also failed: {e_json5}. Returning empty list.\")\n",
        "      return []\n",
        "  \n",
        "  # Ensure the output is list[dict[str, Any]]\n",
        "  if isinstance(parsed_object, list):\n",
        "    # Filter the list to ensure all elements are dictionaries\n",
        "    # result_list = [item for item in parsed_object if isinstance(item, dict)]\n",
        "    # if len(result_list) != len(parsed_object):\n",
        "    #   malformed_items = [item for item in parsed_object if not isinstance(item, dict)]\n",
        "    #   print(f\"Warning (parse_model_response_mpve): Filtered out {len(malformed_items)} non-dictionary item(s) from model response list. Items: {malformed_items}\")\n",
        "    return parsed_object\n",
        "  elif isinstance(parsed_object, dict):\n",
        "    # If a single dictionary is returned, wrap it in a list\n",
        "    print(f\"Warning (parse_model_response_mpve): Parsed model response is a single dictionary. Wrapping it in a list.\")\n",
        "    return [parsed_object]\n",
        "  else:\n",
        "    # If parsed_object is neither a list nor a dict (e.g., int, str, None)\n",
        "    print(f\"Warning (parse_model_response_mpve): Parsed model response is of unexpected type {type(parsed_object)} (value: {str(parsed_object)[:100]}...). Returning empty list.\")\n",
        "    return []\n",
        "\n",
        "\n",
        "def mpv_model_eval_paper(\n",
        "    record_id: str | None,\n",
        "    ground_truth: str,\n",
        "    model_response: str,\n",
        "    eval_prompt: str,\n",
        "    client: Any,\n",
        ") -> dict[str, Any]:\n",
        "  \"\"\"Runs model evaluation on material properties for a single paper.\n",
        "\n",
        "  Args:\n",
        "    record_id: record id or paper id.\n",
        "    ground_truth: ground truth list in str type.\n",
        "    model_response: model response list in str type.\n",
        "    eval_prompt: eval prompt.\n",
        "    client: llm client.\n",
        "\n",
        "  Returns:\n",
        "    model eval response json.\n",
        "  \"\"\"\n",
        "  json_ground_truth = parse_ground_truth_mpve(ground_truth)\n",
        "  json_model_response = parse_model_response_mpve(model_response)\n",
        "  return model_eval_json(\n",
        "      record_id=record_id,\n",
        "      json_ground_truth=json_ground_truth,\n",
        "      json_model_response=json_model_response,\n",
        "      eval_prompt=eval_prompt,\n",
        "      client=client,\n",
        "  )\n",
        "\n",
        "\n",
        "def filter_ground_truth_properties(ground_truth: str) -> list[dict[str, Any]]:\n",
        "  \"\"\"Filters ground truth to only keep the properties we want to evaluate.\n",
        "\n",
        "  Args:\n",
        "    ground_truth: ground truth list in str type.\n",
        "\n",
        "  Returns:\n",
        "    filtered ground truth list.\n",
        "  \"\"\"\n",
        "  json_ground_truth = json5.loads(ground_truth)\n",
        "  filtered_ground_truth = []\n",
        "  valid_property_names = [\n",
        "      \"bandgap\",\n",
        "      \"band gap\",\n",
        "      \"gap energy\",\n",
        "      \"energy gap\",\n",
        "      \"refractive_index\",\n",
        "      \"refractive index\",\n",
        "      \"index of refraction\",\n",
        "      \"n-value\",\n",
        "      \"n value\",\n",
        "  ]\n",
        "  for item in json_ground_truth:\n",
        "    for valid_property_name in valid_property_names:\n",
        "      if valid_property_name in item[\"property_name\"].lower():\n",
        "        filtered_ground_truth.append(item)\n",
        "        break\n",
        "  return filtered_ground_truth\n",
        "\n",
        "\n",
        "def mpve_domain_expert_model_based_eval(\n",
        "    ground_truth: str,\n",
        "    model_response: str,\n",
        "    client: Any | None = None,\n",
        "    eval_prompt: str = _EVAL_PROMPT_FILENAME,\n",
        "    verbose: bool = True,\n",
        "    paper_id: str = \"\",\n",
        ") -> dict[str, Any]:\n",
        "  \"\"\"Runs model based eval on material properties.\n",
        "\n",
        "  Args:\n",
        "    ground_truth: ground truth list in str type.\n",
        "    model_response: model response list in str type.\n",
        "    client: llm client.\n",
        "    eval_prompt: eval prompt.\n",
        "    verbose: whether to print out eval results.\n",
        "\n",
        "  Returns:\n",
        "    eval result.\n",
        "  \"\"\"\n",
        "  if verbose:\n",
        "    print(f\"Model eval started... with paper_id: {paper_id}\")\n",
        "  eval_output_item = mpv_model_eval_paper(\n",
        "      record_id=paper_id,\n",
        "      ground_truth=ground_truth,\n",
        "      model_response=model_response,\n",
        "      eval_prompt=eval_prompt,\n",
        "      client=client,\n",
        "  )\n",
        "  if verbose:\n",
        "    import time\n",
        "    # 输出当前北京时间\n",
        "    beijing_time_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
        "    print(f\"Model eval finished.in {beijing_time_str}\")\n",
        "  eval_result = eval_overall_result(\n",
        "      eval_output_item, verbose=verbose\n",
        "  )\n",
        "  if verbose:\n",
        "    print(\"Eval results:\\n\", eval_result)\n",
        "  return eval_result\n",
        "\n",
        "def load_matsci_prompt(filepath: str) -> str:\n",
        "  \"\"\"Loads matsci prompt.\n",
        "\n",
        "  Args:\n",
        "    filepath: filepath of prompt.\n",
        "\n",
        "  Returns:\n",
        "    Loaded prompt.\n",
        "  \"\"\"\n",
        "  # return resources.GetResource(filepath).decode(\"utf-8\").strip()\n",
        "  with open(filepath, 'r') as file:\n",
        "    text_content = file.read()\n",
        "  return text_content.strip()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Kdc30iBVLY9-"
      },
      "outputs": [],
      "source": [
        "from logging import raiseExceptions\n",
        "def get_annotated_structure_metadata_and_dft_params(\n",
        "    gt_paper_code: str, verbose: int = 0\n",
        ") -> dict[str, list[str]]:\n",
        "  \"\"\"Returns structure metadata and dft params from the ground truth code.\n",
        "\n",
        "  Args:\n",
        "    gt_paper_code: The ground truth code from .py file as a string.\n",
        "    verbose: The verbosity level.\n",
        "  \"\"\"\n",
        "  structures = []\n",
        "  dft_params = []\n",
        "\n",
        "  paper = gt_paper_code\n",
        "  if \"structure_metadata_\" in paper:\n",
        "    parts = paper.split(\"structure_metadata_\")[1:]\n",
        "    whole_parts = [\"structure_metadata_\" + part for part in parts]\n",
        "\n",
        "    for part in whole_parts:\n",
        "      if verbose > 1:\n",
        "        print(\"PART:\\n\", part)\n",
        "      if \"parse_raw(\" not in part:\n",
        "        continue\n",
        "      left, right, *_ = part.split(\"parse_raw(\")\n",
        "      if \"StructureMetadata\" in left:\n",
        "        end_struc = \")\"\n",
        "        if \"')\" in right:\n",
        "          end_struc = \"')\"\n",
        "        elif \"'\\n)\" in right:\n",
        "          end_struc = \"'\\n)\"\n",
        "        struc_json = right.split(end_struc)[0].strip()\n",
        "        if verbose > 0:\n",
        "          print(\"Extracted structure:\\n\", struc_json)\n",
        "        # clean_json = struc_json.replace('NaN', '\"NaN\"')\n",
        "        # struc_json = ast.literal_eval(clean_json)\n",
        "        structures.append(struc_json)\n",
        "\n",
        "  if \"dft_params_\" in paper:\n",
        "    parts = paper.split(\"dft_params_\")[1:]\n",
        "    # print(parts)\n",
        "    whole_parts = [\"dft_params_\" + part for part in parts]\n",
        "\n",
        "    for part in whole_parts:\n",
        "      if verbose > 1:\n",
        "        print(\"PART:\\n\", part)\n",
        "      if \"parse_raw(\" not in part:\n",
        "        continue\n",
        "      left, right, *_ = part.split(\"parse_raw(\")\n",
        "      if \"DFTParameters\" in left:\n",
        "        end_struc = \")\"\n",
        "        if \"')\" in right:\n",
        "          end_struc = \"')\"\n",
        "        elif \"'\\n)\" in right:\n",
        "          end_struc = \"'\\n)\"\n",
        "        dft_params_str = right.split(end_struc)[0].strip()\n",
        "        if verbose > 0:\n",
        "          print(\"Extracted dft_param:\\n\", dft_params_str)\n",
        "        # clean_json = dft_params_str.replace('NaN', '\"NaN\"')\n",
        "        # dft_params_str = ast.literal_eval(clean_json)\n",
        "        dft_params.append(dft_params_str)\n",
        "\n",
        "  gt_struc_jsons = []\n",
        "  for struct_metadata in structures:\n",
        "    gt_json = struct_metadata.split(\"'\")[1]\n",
        "    clean_json = gt_json.replace(\"NaN\", '\"NaN\"')\n",
        "    try:\n",
        "      gt_structure_json = ast.literal_eval(clean_json)\n",
        "      gt_struc_jsons.append(gt_structure_json)\n",
        "    except Exception:  # pylint: disable=broad-exception-caught\n",
        "      gt_struc_jsons.append(clean_json)\n",
        "\n",
        "  gt_dft_params_jsons = []\n",
        "  for dft_param in dft_params:\n",
        "    gt_json = dft_param.split(\"'\")[1]\n",
        "    clean_json = gt_json.replace(\"NaN\", '\"NaN\"')\n",
        "    try:\n",
        "      gt_dft_json = ast.literal_eval(clean_json)\n",
        "      gt_dft_params_jsons.append(gt_dft_json)\n",
        "    except Exception:  # pylint: disable=broad-exception-caught\n",
        "      gt_dft_params_jsons.append(clean_json)\n",
        "\n",
        "  return {\n",
        "      \"structures_metadata\": gt_struc_jsons,\n",
        "      \"dft_params\": gt_dft_params_jsons,\n",
        "  }\n",
        "\n",
        "\n",
        "def preprocess_ground_truth(\n",
        "    ground_truth: str, task_name: str, prompt: str\n",
        ") -> str:\n",
        "  \"\"\"Preprocesses the ground truth before sending to eval.\"\"\"\n",
        "  # Drops the record_ids.\n",
        "  json_gt = json5.loads(ground_truth)\n",
        "  if isinstance(json_gt, dict):\n",
        "    json_gt.pop(\"record_id\", None)\n",
        "    json_gt.pop(\"arxiv_id\", None)\n",
        "    json_gt.pop(\"paper_id\", None)\n",
        "  if isinstance(json_gt, list):\n",
        "    for item in json_gt:\n",
        "      if isinstance(item, dict):\n",
        "        item.pop(\"record_id\", None)\n",
        "        item.pop(\"arxiv_id\", None)\n",
        "        item.pop(\"paper_id\", None)\n",
        "  groundtruth_with_no_ids = json5.dumps(json_gt)\n",
        "  # Preprocess for dft metadata tasks.\n",
        "  if task_name == \"dft\" and prompt == \"extract_dft_metadata_1_shot\":\n",
        "    processed = get_annotated_structure_metadata_and_dft_params(\n",
        "        groundtruth_with_no_ids\n",
        "    )[\"dft_params\"]\n",
        "  elif task_name == \"dft\" and prompt == \"extract_structure_data_1_shot\":\n",
        "    processed = get_annotated_structure_metadata_and_dft_params(\n",
        "        groundtruth_with_no_ids\n",
        "    )[\"structures_metadata\"]\n",
        "  else:\n",
        "    processed = groundtruth_with_no_ids\n",
        "  return str(processed)\n",
        "\n",
        "\n",
        "def read_task_ground_truth_and_response(\n",
        "    ground_truth_path: str,\n",
        "    model_response_path: str,\n",
        ") -> Tuple[str, str, str]:\n",
        "  \"\"\"Reads in the ground truth and response for all tasks.\"\"\"\n",
        "  try:\n",
        "    # Gets the ground truth.\n",
        "    with open(ground_truth_path, \"r\") as f:\n",
        "      ground_truth_info = f.read()\n",
        "\n",
        "    model_response = \"\"\n",
        "    inf_prompt = \"\"\n",
        "    if os.path.exists(model_response_path):\n",
        "      with open(model_response_path, \"r\") as f:\n",
        "        full_model_response = json5.loads(f.read())\n",
        "        if \"response_text\" in full_model_response:\n",
        "          model_response = full_model_response[\"response_text\"]\n",
        "        else:\n",
        "          raise ValueError(\n",
        "              f\"ERROR: The succeeded response for {model_response_path} does not contain response_text.\"\n",
        "          )\n",
        "        if 'pdb' in ground_truth_path:\n",
        "          if 'prompt_text' in full_model_response:\n",
        "            inf_prompt = full_model_response[\"prompt_text\"]\n",
        "          else:\n",
        "            raise ValueError(\n",
        "                f\"ERROR: The succeeded response for {model_response_path} does not contain prompt_text.\"\n",
        "            )\n",
        "\n",
        "    failed_model_response = model_response_path.replace(\"success\", \"failure\")\n",
        "    # Gets the response.\n",
        "    exception_message = \"\"\n",
        "    if os.path.exists(failed_model_response):\n",
        "      with open(failed_model_response, \"r\") as f:\n",
        "        full_model_response = json5.loads(f.read())\n",
        "        if \"exception_message\" in full_model_response:\n",
        "          exception_message = full_model_response[\"exception_message\"]\n",
        "        elif \"command-r-plus\" in failed_model_response and \"response_text\" in full_model_response:\n",
        "          exception_message = full_model_response[\"response_text\"]\n",
        "        else:\n",
        "          raise ValueError(\n",
        "              f\"ERROR: The failure response for {failed_model_response} does not contain exception message.\"\n",
        "          )\n",
        "\n",
        "    return ground_truth_info, model_response, exception_message, inf_prompt\n",
        "  except Exception as e:\n",
        "    print(f\"ERROR: {e}\")\n",
        "    raise Exception(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brIrWo-gPzB8"
      },
      "source": [
        "## static configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sTqvJmb_BxBa"
      },
      "outputs": [],
      "source": [
        "# _SHARED_METRCS = [get_rouge_score, get_bert_score]\n",
        "# _FULL_ADDITIONAL_METRICS = {\n",
        "#     \"pdb\": {\n",
        "#         \"reconstruct_protein_amino_acid_sequence_0_shot\": {\n",
        "#             pdb_reconstruction_eval\n",
        "#         },\n",
        "#     },\n",
        "#     \"mpve\": {\n",
        "#         \"mat_paper_to_property_1_shot\": {\n",
        "#             get_lmsim_score_mpve\n",
        "#         },\n",
        "#         \"mat_paper_to_property_1_shot_exclude_trivia\": {\n",
        "#             get_lmsim_score_mpve\n",
        "#         },\n",
        "#         \"mat_paper_to_property_1_shot_bandgap_refractive\": {\n",
        "#             get_lmsim_score_mpve\n",
        "#         }\n",
        "#     },\n",
        "#     \"dft\": {\n",
        "#         \"extract_structure_data_1_shot\": {\n",
        "#             get_lmsim_score_dft\n",
        "#         },\n",
        "#         \"extract_dft_metadata_1_shot\": {\n",
        "#             get_lmsim_score_dft\n",
        "#         },\n",
        "#     },\n",
        "#     \"biogr\": {\n",
        "#         \"georeference_image_0_shot\": {\n",
        "#             biodiversity_georeferencing_eval\n",
        "#         }\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# _PRIMARY_ADDITIONAL_METRICS = {\n",
        "#     \"pdb\": {\n",
        "#         \"reconstruct_protein_amino_acid_sequence_0_shot\": {\n",
        "#             pdb_reconstruction_eval\n",
        "#         },\n",
        "#     },\n",
        "#     \"biogr\": {\n",
        "#         \"georeference_image_0_shot\": {\n",
        "#             biodiversity_georeferencing_eval\n",
        "#         }\n",
        "#     }\n",
        "\n",
        "# }\n",
        "\n",
        "# _LLM_LIST = [\"command-r-plus\", \"longllama\", \"mixtral-gcp\",\n",
        "#              \"gemini-1.5-flash-latest\", \"gemini-1.0-pro\", \"gemini-1.5-pro-latest\",\n",
        "#              \"gpt-4o\", \"claude-3-opus-20240229\", 'gemini-2.0-flash-latest',\n",
        "#              \"deepseek-r1\",\"doubao-1.5-thinking-pro\"\n",
        "#              ]\n",
        "\n",
        "# _BIOGR_EXCLUDE_LLM = [\"command-r-plus\", \"longllama\", \"mixtral-gcp\"]\n",
        "# _TASK_EVAL_CONFIGS = {\n",
        "#     \"hfe\": {\n",
        "#         \"extract_hamiltonian_0_shot\": {\n",
        "#         }\n",
        "#     },\n",
        "#     \"hfd\": {\n",
        "#         \"derivation_prompt\": {\n",
        "#         }\n",
        "#     },\n",
        "#     \"qecc_65\": {\n",
        "#         \"describe_code_in_paper\": {\n",
        "#         }\n",
        "#     },\n",
        "#     \"pdb\": {\n",
        "#         \"reconstruct_protein_amino_acid_sequence_0_shot\": {\n",
        "#         }\n",
        "#     },\n",
        "#     \"mpve\": {\n",
        "#         \"mat_paper_to_property_1_shot\": {\n",
        "#         },\n",
        "#         \"mat_paper_to_property_1_shot_bandgap_refractive\": {\n",
        "#         },\n",
        "#         \"mat_paper_to_property_1_shot_exclude_trivia\": {\n",
        "#         },\n",
        "#     },\n",
        "#     \"dft\": {\n",
        "#         \"write_code_for_paper_0_shot\": {\n",
        "#         },\n",
        "#         \"extract_structure_data_1_shot\": {\n",
        "#         },\n",
        "#         \"extract_dft_metadata_1_shot\": {\n",
        "#         },\n",
        "#     },\n",
        "#     \"geo\": {\n",
        "#         \"extract_dataset_from_geo_papers_0_shot\": {\n",
        "#         }\n",
        "#     },\n",
        "#     \"biogr\": {\n",
        "#         \"georeference_image_0_shot\": {\n",
        "#         }\n",
        "#     },\n",
        "# }\n",
        "# all_ids_per_task = {'pdb': ['1A12', '1A33', '1AIL', '1AOA', '1AQA', '1AZS', '1BL8', '1BM8', '1BXO', '1CC5', '1CDK', '1CGF', '1CLL', '1CTF', '1DIN', '1DXX', '1E9H', '1EST', '1F9V', '1G6X', '1GOF', '1GP1', '1HCG', '1HHO', '1HNF', '1HNV', '1IAV', '1IGM', '1IGT', '1JM1', '1KXQ', '1M03', '1M17', '1M8Q', '1MBG', '1MBO', '1MHC', '1NKO', '1POH', '1PRC', '1R09', '1RCP', '1RGS', '1SBT', '1SU4', '1TIT', '1TNK', '1UBQ', '2A99', '2ACE', '2AYN', '2J1N', '2POR', '2R6G', '3ADN', '3C7E', '3LCK', '3R2E', '4CPA', '5CPA', '5HZN', '7B3N', '7L1E', '7V8O'],\n",
        "#                     'biogr': ['10212153_1', '260729_2', '531730_1', '556058_2', '563682_1', '564487_1', '575983_2', '578304_2', '583537_1', '585031_1', '587419_1', '590257_1', '591038_1', '592166_1', '592526_1', '592805_4', '594665_1', 'S0048969724009641_1', 'S1470160X22006951_1', 'a_decade_of_submersible_observations_1', 'a_new_species_of_river_1', 'a_preliminary_investigation_of_the_3', 'a_simple_genetic_method_to_1', 'a_small_warm_tributary_provides_1', 'abundance_of_longbilled_curlews_on_1a', 'an_overview_of_marine_biodiversity_3', 'an_overview_of_marine_biodiversity_8', 'assessment_of_ambystomatid_salamander_populations_1', 'assessment_of_potential_recovery_viability_2', 'availability_of_supplemental_corn_for_1', 'barriers_to_gene_flow_in_1', 'baseline_assessments_for_coral_reef_1', 'bat_predation_by_spiders_1', 'biotic_assemblages_of_gelatinous_zooplankton_3', 'bird_monitoring_at_effigy_mounds_1', 'birds_of_the_kilbuck_and_1', 'breeding_population_size_of_the_1', 'ceratonova_shasta_infection_in_lower_1', 'characterization_of_a_developing_recreational_1', 'chewing_lice_of_swan_geese_1', 'comparison_of_endoparasite_abundance_and_1', 'comparisons_of_walleye_fecundity_before_1', 'conservation_genetics_of_the_endangered_1', 'cooccurrence_of_ecologically_similar_species_1', 'deep_vs_shallow_gps_tags_1', 'density_of_axis_deer_in_1', 'despite_regional_variation_gymnorhinus_cyanocephalus_1', 'distribution_abundance_and_breeding_activities_1', 'distribution_and_abundance_of_least_3', 'distribution_morphology_and_karyotype_of_1', 'diurnal_human_activity_and_introduced_1', 'diving_patterns_and_foraging_locations_1', 'dna_barcoding_the_native_flowering_5', 'documentation_of_a_probable_spawning_1', 'ece310733_1', 'energy_density_of_three_prosopium_1', 'environment_affects_sucker_catch_rate_1', 'evaluating_spatial_coverage_of_the_1', 'evening_bats_captured_in_a_1', 'expansion_of_smallmouth_bass_distribution_1', 'extreme_wildlife_declines_and_concurrent_1', 'fecal_genotyping_to_estimate_small_1', 'first_record_of_paronatrema_vaginicola_1', 'fish_predation_by_semiaquatic_spiders_1', 'foraging_ecology_of_southern_sea_1', 'four_centuries_of_change_in_1', 'global_conservation_priorities_for_marine_4', 'habitat_suitability_assessment_for_tule_4', 'habitat_use_and_reproductive_success_1', 'hawaiian_hoary_bat_lasiurus_cinereus_1', 'hiding_in_plain_sight_federally_1', 'high_similarity_in_winter_diet_1', 'impacts_of_the_czu_lightning_1', 'incidental_take_of_giant_sea_7', 'incorporating_expanded_sampling_into_an_1', 'inventory_of_eelgrass_zostera_marina_1', 'jwmg22383_1', 'larval_and_juvenile_longfin_smelt_1', 'leveraging_angler_effort_to_inform_1', 'longterm_occupancy_monitoring_reveals_value_1', 'machine_learning_to_understand_patterns_1', 'macrohabitat_suitability_model_for_the_1', 'macroscale_effects_of_the_monument_1', 'marine_biodiversity_in_the_atlantic_4', 'microhabitat_characteristics_and_management_of_1', 'monitoring_fiveneedle_pine_on_bureau_1', 'monitoring_nesting_waterbirds_for_the_1', 'monitoring_questing_winter_tick_abundance_1', 'movement_patterns_of_two_bat_1', 'natal_contributions_of_kokanee_salmon_1', 'occurrence_of_a_reproducing_wild_3', 'occurrence_of_batrachochytrium_dendrobatidis_in_1', 'onceiconic_pismo_clams_persist_in_1', 'patterns_of_florida_bonneted_bat_2', 'population_and_spatial_dynamics_of_1', 'population_density_and_habitat_selection_2', 'population_genomic_surveys_for_six_1', 'postfire_survival_of_the_threatened_4', 'rangewide_genetic_analysis_of_an_1', 'rapid_population_decline_in_mckays_1', 'recovering_the_lost_potential_of_5', 'relative_influence_of_environmental_factors_1', 'rescuing_and_monitoring_white_sturgeon_2', 'revealing_biases_in_insect_observations_5', 'review_of_considerations_for_restoration_1', 'road_and_highway_undercrossings_as_1', 'roseate_tern_breeding_dispersal_and_1', 's41598022209644_1', 's41598023276709_1', 's4159802334533_1', 'sampling_duration_and_season_recommendations_1', 'sea_level_rise_vulnerability_assessment_1', 'seacliff_bedstraw_galium_buxifolium_patterns_4', 'seasonal_and_spatial_distribution_of_1', 'spatial_relationships_and_mesoscale_habitat_1', 'spatial_variation_in_density_of_1', 'status_and_distribution_of_arroyo_2', 'status_assessment_of_the_endangered_1', 'status_of_landbirds_in_the_1', 'striped_bass_movement_in_a_1', 'syntopy_in_california_redlegged_and_1', 'testing_a_singlevisit_sampling_approach_1', 'the_biodiversity_of_the_mediterranean_5', 'the_first_occurrence_of_the_3', 'the_importance_of_subarctic_intertidal_1', 'the_lion_in_west_africa_1', 'time_series_modeling_of_rainfall_1', 'trace_elements_in_blood_of_1', 'travel_management_planning_for_wildlife_1', 'trends_in_amphibian_occupancy_in_1', 'tricolored_blackbird_survey_methods_1', 'tule_elk_selection_of_surface_1', 'unintended_consequences_of_species_translocations_1', 'us_atlantic_and_gulf_of_1', 'use_of_aerial_distance_sampling_1', 'utilizing_the_timetoevent_framework_to_1', 'validating_a_nonlethal_method_of_1', 'western_purple_martin_progne_subis_1'],\n",
        "#                     'geo': ['00000', '14614a88b3e44e601c5cf8f71b5e07ca989beb0b', '213d2232a49507f81b4e17e50de7675c88fbc672', '33b0925f7681f3199a5d075324e7f3c5e33f2c76', '41f20bb04729a55ca9c2eaf579adf3ed5729044b', '54a9885771350f38135f30f43ef874e0a30be07b', '5c37c2aa2e108e17e37c6db29a4e5afe6a811119', '7dc47696eb876d85a3dfc6884f61fa8832d5e5e8', '83a1a10e3a2416e1d93bc3dbb482db4ccb707eda', '850ca33e8c1853c1735da63073ec3910bce91ddc', '9bdabc37e4af91c4fb53e205502204b510e3b972', 'a09b49e5f2c6b818e479bd29343eae9005f8ca26', 'ab6d648944f306fa1e2d275115b94d36478d9d2a', 'b90358f971e19a60c305acff2867b89dd197fdf6', 'c3a3a5a24206a9b38d9f4727f78cc8f323e398b2', 'e57ae1987bce88add50696843c8979456ce55561', 'e88aa0bccedc5f07bfee8f2db7a85351e65ec24a', 'e900993457d4d256cbfbe8a7527b6745f130a98e', 'e9c8932d5fcdf067821f8bf24b7462e5c7f73054'], 'hfd': ['1010.1819', '1106.6060', '1208.0116', '1310.2674', '1508.00296', '1812.04213', '2004.04168', '2008.08998', '2012.04554', '2108.02159', '2110.11330', '2111.01152', '2112.07523', '2308.03843', '2308.07488'], 'hfe': ['1010.1819', '1112.4222', '1202.4956', '1206.0608', '1208.0116', '1212.5363', '1401.2167', '1506.01488', '1507.06420', '1510.06887', '1512.02398', '1601.00996', '1812.04213', '1908.05417', '2007.15166', '2008.08998', '2102.13507', '2108.02159', '2111.09813', '2112.07523', '2206.10024', '2208.07620', '2209.15374', '2210.06674', '2210.08025', '2210.14517', '2302.04864', '2303.09821', '2303.18025', '2306.02127', '2306.12486', '2307.03793', '2307.04307', '2307.07531', '2307.11810', '2308.01997', '2308.03843', '2311.13191'], 'qecc_65': ['1501.07779', '1502.05267', '1503.06237', '1503.08800', '1505.02576', '1602.00008', '1603.04442', '1604.07925', '1703.02973', '1707.02308', '1708.08474', '1709.04471', '1709.08658', '1710.04631', '1712.07666', '1801.05897', '1802.07419', '1805.01474', '1809.09801', '1903.03937', '1906.11394', '1907.09528', '1910.10746', '2003.02717', '2007.09154', '2007.12152', '2008.09495', '2009.03921', '2010.06628', '2106.02649', '2107.02194', '2110.11510', '2112.01446', '2201.07802', '2203.00103', '2203.16534', '2209.11405', '2210.10808', '2210.16957', '2212.09935', '2303.02432', '2303.04798', '2306.11621', '2309.16503', '2311.07679', '2311.08653', '2311.13040', '2312.04522', '2402.07476', 'cond-mat_0010440', 'cond-mat_0607736', 'cond-mat_9707273', 'cs_0509062', 'quant-ph_0008040', 'quant-ph_0210097', 'quant-ph_0502086', 'quant-ph_0605138', 'quant-ph_0701020', 'quant-ph_0702075', 'quant-ph_9703002', 'quant-ph_9705052', 'quant-ph_9711021', 'quant-ph_9711049', 'quant-ph_9810055', 'quant-ph_9906114'], 'dft': ['2023_09_22_01b9cdba467fd7882e42g', '2023_09_22_07b4d66e23971ccb85c0g', '2023_09_22_0ce1b5ea9a8637db5435g', '2023_09_22_109a6cd5d015ce89e7f3g', '2023_09_22_12cb692c6b82606615a6g', '2023_09_22_13bcf90c3ef43f1413deg', '2023_09_22_14ab0a44fc8fc33fa338g', '2023_09_22_14ddd903c0b77b5e50c2g', '2023_09_22_182e0132c513bc81a414g', '2023_09_22_1a3a4803f9d9cec16d38g', '2023_09_22_1af7e6342ebcf1ce3ea5g', '2023_09_22_1b00ed3a142a7a1b2582g', '2023_09_22_1def59bea80d6e9f67ccg', '2023_09_22_22c19cfc32d2575f9a52g', '2023_09_22_24d7d9ed97e042af9f29g', '2023_09_22_2d35eebe2e85cecf4103g', '2023_09_22_2d44dda253969d6ce7f6g', '2023_09_22_2e5b7c3f50b6a643e33ag', '2023_09_22_2e9b1b9bffe7fd47b18fg', '2023_09_22_39b7d4444fd6ff852b12g', '2023_09_22_3ae1fab1e33569c30b8dg', '2023_09_22_430c3ddffa99af6a2545g', '2023_09_22_433b6bb3bfc2391f7300g', '2023_09_22_48fb54662c601e035a91g', '2023_09_22_49d6cc9c5e7a469afee0g', '2023_09_22_4eeeb89f0a6f52e58610g', '2023_09_22_4ef39bb4116ac1dad9e9g', '2023_09_22_5237e17b3b341fecc9d9g', '2023_09_22_54050a76c33463a8157fg', '2023_09_22_55f975d508230ef05caeg', '2023_09_22_63a752c620bbc784200cg', '2023_09_22_67e874a3c664208e2d2fg', '2023_09_22_6ac063e0fb85cfc10dd0g', '2023_09_22_70326f83ce0dcec87b50g', '2023_09_22_7433a6c7542334063731g', '2023_09_22_77a765fcab6029c666b4g', '2023_09_22_799ee1c298d190145c70g', '2023_09_22_7c5bbe7e076779b790ccg', '2023_09_22_7c76b066edb1f6f53739g', '2023_09_22_7dab45b11a3ede362147g', '2023_09_22_8181c4d6fa78a2ea82dag', '2023_09_22_81b3dfeb3597db5200c5g', '2023_09_22_84af4abb781aeead403eg', '2023_09_22_87d405f182ae3706ea0cg', '2023_09_22_8b46d7b3e561e7f28495g', '2023_09_22_8df0b56e310badc55de3g', '2023_09_22_900c617369212d6bc72fg', '2023_09_22_910aca2a500d3bf9bf47g', '2023_09_22_980121c407cbdaa46afdg', '2023_09_22_984f5b905c02b6f21733g', '2023_09_22_995805c76f676dddab4fg', '2023_09_22_9a007c3865721f379b39g', '2023_09_22_9a591ebf98377fd0ebe2g', '2023_09_22_9e2bc88db643c6ba8aa0g', '2023_09_22_a0271d2dc7b0f2506498g', '2023_09_22_b0501f9057db320b8ad9g', '2023_09_22_b2865949a80ad08a2835g', '2023_09_22_bba019fc933fc84ad347g', '2023_09_22_cb81fee8faa69f4d7078g', '2023_09_22_cc792b66f9a5779f9798g', '2023_09_22_cff3389f103b8f7971d0g', '2023_09_22_d84d81c022c4b2981048g', '2023_09_22_d90e94cbd96e4b6ddb8bg', '2023_09_22_dd9f0f77c116dc99583ag', '2023_09_22_ddfb75e0fb765dc682bbg', '2023_09_22_e06d11a6e698afe5f2d7g', '2023_09_22_e32d0198a1f3dddb5ba2g', '2023_09_22_e69f3d7ce6c4ff487115g', '2023_09_22_e8d1bc2fb9f3dce5f341g', '2023_09_22_edae82c7fbe0c4062118g', '2023_09_22_efbe854b8da1545fbe9bg', '2023_09_22_f752dc9d5ac72657e3f5g', '2023_09_22_f7714e3a468c91c6f56ag', '2023_09_22_f8875eb68affb0a6cb2bg'], 'mpve': ['10222315', '11093908', '11181068', '12841719', '135893324', '137261967', '137362119', '15804005', '17645319', '2837337', '317542', '53384093', '53519111', '55005437', '6183251', '68518', '97574650']}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH5McmXXBnEp"
      },
      "source": [
        "# get full eval results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGlcBn9IVDJd"
      },
      "source": [
        "## calculates metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NU1APUUhINwa"
      },
      "outputs": [],
      "source": [
        "# Set this to True if you want all metrics including LLMSim scores.\n",
        "runs_full_metric = False #@param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cIgjlh5IVGlW"
      },
      "outputs": [],
      "source": [
        "# results_json = {}\n",
        "# for task in _TASK_EVAL_CONFIGS.keys():\n",
        "#   results_json[task] = {}\n",
        "#   for prompt in _TASK_EVAL_CONFIGS[task]:\n",
        "#     results_json[task][prompt] = {}\n",
        "#     for llm_name in _LLM_LIST:\n",
        "#       results_json[task][prompt][llm_name] = {}\n",
        "#       for record_id in all_ids_per_task[task]:\n",
        "#         print(f\"task: {task}, prompt: {prompt}, llm_name: {llm_name}, record_id: {record_id}\")\n",
        "#         results_json[task][prompt][llm_name][record_id] = {}\n",
        "#         print(f\"{task}, {prompt}, {llm_name}, {record_id}\")\n",
        "#         gt_path = os.path.join(file_root_path, \"data\", task, \"ground_truth\", record_id + \".json\")\n",
        "#         model_response_path = os.path.join(file_root_path, \"inference_outputs\", task, prompt, llm_name, \"success\", record_id + \".json\")\n",
        "#         try:\n",
        "#           ground_truth_info, model_response, exception_message, inf_prompt = read_task_ground_truth_and_response(gt_path, model_response_path)\n",
        "#           ground_truth_info = preprocess_ground_truth(ground_truth_info, task, prompt)\n",
        "#         except Exception as e:\n",
        "#           print(f\"ERROR: {e}\")\n",
        "#           continue\n",
        "#         if (not task == 'pdb' and model_response) or (task == 'pdb' and model_response and inf_prompt):\n",
        "\n",
        "#           full_additional_metrics = list(_FULL_ADDITIONAL_METRICS[task][prompt]) if task in _FULL_ADDITIONAL_METRICS  and prompt in _FULL_ADDITIONAL_METRICS[task] else []\n",
        "#           primary_additional_metrics = list(_PRIMARY_ADDITIONAL_METRICS[task][prompt]) if task in _PRIMARY_ADDITIONAL_METRICS  and prompt in _PRIMARY_ADDITIONAL_METRICS[task] else []\n",
        "#           additional_metrics = full_additional_metrics if runs_full_metric else primary_additional_metrics\n",
        "#           all_metrics = additional_metrics + _SHARED_METRCS\n",
        "#           for metric in all_metrics:\n",
        "#             try:\n",
        "#               if task == 'pdb' and metric in additional_metrics:\n",
        "#                 res = metric(model_response, ground_truth_info, inf_prompt)\n",
        "#               else:\n",
        "#                 res = metric(model_response, ground_truth_info)\n",
        "#               print(res)\n",
        "#               results_json[task][prompt][llm_name][record_id].update(res)\n",
        "#             except Exception as e:\n",
        "#               print(\"##### ERROR #####\")\n",
        "#               print(e)\n",
        "#               print(f\"##### skipped {task}, {prompt}, {llm_name}, {record_id} #####\")\n",
        "#               continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKD1HSXD31qA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['17645319', '53519111', '53384093', '317542', '97574650', '135893324', '68518', '6183251', '2837337', '137362119', '10222315', '11093908', '55005437', '15804005', '137261967', '12841719', '11181068']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model eval started... with paper_id: 17645319\n",
            "using llm to parse\n"
          ]
        }
      ],
      "source": [
        "import concurrent.futures\n",
        "if TASK_NAME == \"MPVE\":\n",
        "  inference_path=os.path.join(file_root_path, \"inference\",\"multi_runs/current/mpve/mat_paper_to_property_1_shot_exclude_trivia\",INFER_MODEL_NAME,\"run_1/success\")\n",
        "  # print(os.listdir(inference_path))\n",
        "  # 获得去掉后缀的文件名\n",
        "  paper_ids = [os.path.splitext(file)[0] for file in os.listdir(inference_path)]\n",
        "  print(paper_ids)\n",
        "  ground_truth_path = os.path.join(file_root_path, \"data\", \"mpve\", \"ground_truth\")\n",
        "elif TASK_NAME == \"DFT\":\n",
        "  inference_path=os.path.join(file_root_path, \"inference\",\"multi_runs/current/dft/extract_dft_metadata_1_shot\",INFER_MODEL_NAME,\"run_1/success\")\n",
        "  # print(os.listdir(inference_path))\n",
        "  # 获得去掉后缀的文件名\n",
        "  paper_ids = [os.path.splitext(file)[0] for file in os.listdir(inference_path)]\n",
        "  print(paper_ids)\n",
        "  ground_truth_path = os.path.join(file_root_path, \"data\", \"dft\", \"ground_truth\")\n",
        "# paper_ids=['11093908']\n",
        "\n",
        "# print(os.listdir(ground_truth_path))\n",
        "# for paper_id in paper_ids:\n",
        "#   # print(paper_id)\n",
        "#   gt_path = os.path.join(ground_truth_path, paper_id + \".json\")\n",
        "#   model_response_path = os.path.join(inference_path, paper_id + \".json\")\n",
        "#   try:\n",
        "#     ground_truth_info, model_response, exception_message, inf_prompt = read_task_ground_truth_and_response(gt_path, model_response_path)\n",
        "#     ground_truth_info = preprocess_ground_truth(ground_truth_info, \"mpve\", \"mat_paper_to_property_1_shot_exclude_trivia\")\n",
        "#   except Exception as e:\n",
        "#     print(f\"ERROR: {e}\")\n",
        "#     continue\n",
        "#   if (not \"mpve\" == 'pdb' and model_response) or (\"mpve\" == 'pdb' and model_response and inf_prompt):\n",
        "#     res = get_lmsim_score_mpve(model_response, ground_truth_info)\n",
        "#     print(res)\n",
        "\n",
        "def process_paper_id(paper_id):\n",
        "  gt_path = os.path.join(ground_truth_path, paper_id + \".json\")\n",
        "  model_response_path = os.path.join(inference_path, paper_id + \".json\")\n",
        "  try:\n",
        "    ground_truth_info, model_response, exception_message, inf_prompt = read_task_ground_truth_and_response(gt_path, model_response_path)\n",
        "    if TASK_NAME == \"MPVE\":\n",
        "      ground_truth_info = preprocess_ground_truth(ground_truth_info, \"mpve\", \"mat_paper_to_property_1_shot_exclude_trivia\")\n",
        "    elif TASK_NAME == \"DFT\":\n",
        "      ground_truth_info = preprocess_ground_truth(ground_truth_info, \"dft\", \"extract_dft_metadata_1_shot\")\n",
        "      # ground_truth_info = preprocess_ground_truth(ground_truth_info, \"dft\", \"dft_structure_eval_output_1_shot\")  \n",
        "    else:\n",
        "      raise ValueError(f\"ERROR: Unsupported task name {TASK_NAME}.\")\n",
        "  except Exception as e:\n",
        "    print(f\"ERROR: {e}\")\n",
        "    return None\n",
        "  if (not \"mpve\" == 'pdb' and model_response) or (\"mpve\" == 'pdb' and model_response and inf_prompt):\n",
        "    if TASK_NAME == \"MPVE\":\n",
        "      res = get_lmsim_score_mpve(model_response, ground_truth_info,paper_id=paper_id)\n",
        "    elif TASK_NAME == \"DFT\":\n",
        "      res = get_lmsim_score_dft(model_response, ground_truth_info,paper_id=paper_id)\n",
        "    return res\n",
        "  return None\n",
        "#使用ThreadPoolExecutor来并行处理文件\n",
        "# paper_ids=[\"53519111\"]\n",
        "# paper_ids = paper_ids[:1]\n",
        "# paper_ids=[\"17645319\"]\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=32) as executor:\n",
        "  results = list(executor.map(process_paper_id, paper_ids))\n",
        "# results = []\n",
        "# error_records = []\n",
        "# for paper_id in paper_ids:\n",
        "#   result = process_paper_id(paper_id)\n",
        "#   results.append(result)\n",
        "# 处理结果\n",
        "for paper_id, result in zip(paper_ids, results):\n",
        "  if result is not None:\n",
        "    print(f\"Paper ID: {paper_id}, Result: {result}\")\n",
        "  else: \n",
        "    print(f\"Paper ID: {paper_id} procNoneessing failed.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 保存评估的结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'num_match': 2, 'num_ground_truth': 3, 'num_model_response': 2, 'precision': 1.0, 'recall': 0.6666666666666666, 'f1': 0.8, 'paper_id': '2023_09_22_55f975d508230ef05caeg'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 1, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_1def59bea80d6e9f67ccg'}\n",
            "{'num_match': 2, 'num_ground_truth': 5, 'num_model_response': 2, 'precision': 1.0, 'recall': 0.4, 'f1': 0.5714285714285715, 'paper_id': '2023_09_22_14ddd903c0b77b5e50c2g'}\n",
            "{'num_match': 1, 'num_ground_truth': 5, 'num_model_response': 2, 'precision': 0.5, 'recall': 0.2, 'f1': 0.28571428571428575, 'paper_id': '2023_09_22_430c3ddffa99af6a2545g'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 1, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_7c5bbe7e076779b790ccg'}\n",
            "{'num_match': 6, 'num_ground_truth': 10, 'num_model_response': 5, 'precision': 1.0, 'recall': 0.6, 'f1': 0.7499999999999999, 'paper_id': '2023_09_22_2d44dda253969d6ce7f6g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_22c19cfc32d2575f9a52g'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_1a3a4803f9d9cec16d38g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_54050a76c33463a8157fg'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_799ee1c298d190145c70g'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_e06d11a6e698afe5f2d7g'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_b0501f9057db320b8ad9g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_1af7e6342ebcf1ce3ea5g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_109a6cd5d015ce89e7f3g'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 1, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_9a007c3865721f379b39g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_01b9cdba467fd7882e42g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_12cb692c6b82606615a6g'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 5, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_24d7d9ed97e042af9f29g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_dd9f0f77c116dc99583ag'}\n",
            "{'num_match': 4, 'num_ground_truth': 9, 'num_model_response': 2, 'precision': 1.0, 'recall': 0.4444444444444444, 'f1': 0.6153846153846153, 'paper_id': '2023_09_22_b2865949a80ad08a2835g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_900c617369212d6bc72fg'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_f752dc9d5ac72657e3f5g'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 3, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_84af4abb781aeead403eg'}\n",
            "{'num_match': 4, 'num_ground_truth': 9, 'num_model_response': 2, 'precision': 1.0, 'recall': 0.4444444444444444, 'f1': 0.6153846153846153, 'paper_id': '2023_09_22_4eeeb89f0a6f52e58610g'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_4ef39bb4116ac1dad9e9g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_81b3dfeb3597db5200c5g'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_995805c76f676dddab4fg'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_77a765fcab6029c666b4g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_efbe854b8da1545fbe9bg'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 1, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_f7714e3a468c91c6f56ag'}\n",
            "{'num_match': 2, 'num_ground_truth': 5, 'num_model_response': 2, 'precision': 1.0, 'recall': 0.4, 'f1': 0.5714285714285715, 'paper_id': '2023_09_22_5237e17b3b341fecc9d9g'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 1, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_8b46d7b3e561e7f28495g'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 1, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_edae82c7fbe0c4062118g'}\n",
            "{'num_match': 5, 'num_ground_truth': 7, 'num_model_response': 2, 'precision': 1.0, 'recall': 0.7142857142857143, 'f1': 0.8333333333333333, 'paper_id': '2023_09_22_e32d0198a1f3dddb5ba2g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_d90e94cbd96e4b6ddb8bg'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_2e9b1b9bffe7fd47b18fg'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 2, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_d84d81c022c4b2981048g'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_0ce1b5ea9a8637db5435g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_49d6cc9c5e7a469afee0g'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 2, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_2d35eebe2e85cecf4103g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_2e5b7c3f50b6a643e33ag'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_e8d1bc2fb9f3dce5f341g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_cb81fee8faa69f4d7078g'}\n",
            "{'num_match': 10, 'num_ground_truth': 13, 'num_model_response': 6, 'precision': 1.0, 'recall': 0.7692307692307693, 'f1': 0.8695652173913044, 'paper_id': '2023_09_22_39b7d4444fd6ff852b12g'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_7dab45b11a3ede362147g'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 2, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_6ac063e0fb85cfc10dd0g'}\n",
            "{'num_match': 4, 'num_ground_truth': 8, 'num_model_response': 2, 'precision': 1.0, 'recall': 0.5, 'f1': 0.6666666666666666, 'paper_id': '2023_09_22_433b6bb3bfc2391f7300g'}\n",
            "{'num_match': 4, 'num_ground_truth': 9, 'num_model_response': 3, 'precision': 1.0, 'recall': 0.4444444444444444, 'f1': 0.6153846153846153, 'paper_id': '2023_09_22_ddfb75e0fb765dc682bbg'}\n",
            "{'num_match': 0, 'num_ground_truth': 11, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_7433a6c7542334063731g'}\n",
            "{'num_match': 0, 'num_ground_truth': 1, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_980121c407cbdaa46afdg'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_910aca2a500d3bf9bf47g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_13bcf90c3ef43f1413deg'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_cff3389f103b8f7971d0g'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 2, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_9a591ebf98377fd0ebe2g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_e69f3d7ce6c4ff487115g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_14ab0a44fc8fc33fa338g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_182e0132c513bc81a414g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_70326f83ce0dcec87b50g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_bba019fc933fc84ad347g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_7c76b066edb1f6f53739g'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 1, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_cc792b66f9a5779f9798g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_8181c4d6fa78a2ea82dag'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_f8875eb68affb0a6cb2bg'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_67e874a3c664208e2d2fg'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_48fb54662c601e035a91g'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 1, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_1b00ed3a142a7a1b2582g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_63a752c620bbc784200cg'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 1, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_a0271d2dc7b0f2506498g'}\n",
            "{'num_match': 0, 'num_ground_truth': 6, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_3ae1fab1e33569c30b8dg'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_07b4d66e23971ccb85c0g'}\n",
            "{'num_match': 0, 'num_ground_truth': 5, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_9e2bc88db643c6ba8aa0g'}\n",
            "{'num_match': 0, 'num_ground_truth': 3, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_984f5b905c02b6f21733g'}\n",
            "{'num_match': 0, 'num_ground_truth': 0, 'num_model_response': 1, 'precision': 0.0, 'recall': 0, 'f1': 0.0, 'paper_id': '2023_09_22_87d405f182ae3706ea0cg'}\n",
            "{'num_match': 0, 'num_ground_truth': 7, 'num_model_response': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'paper_id': '2023_09_22_8df0b56e310badc55de3g'}\n"
          ]
        }
      ],
      "source": [
        "# 给rusults加上paper_id，整体保存为一个json文件，存到inference_path\n",
        "import json\n",
        "for result,paper_id in zip(results, paper_ids):\n",
        "    if result is not None:\n",
        "        result[\"paper_id\"] = paper_id\n",
        "        print(result)\n",
        "        # result[\"eval_model\"] = EVAL_MODEL_NAME\n",
        "    else:\n",
        "        print(f\"Paper ID: {paper_id} processing failed.\")\n",
        "# Save the results to a JSON file\n",
        "\n",
        "output_file = os.path.join(inference_path, \"..\",f\"{INFER_MODEL_NAME} results.json\")\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(results, f, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 保存平均结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paper ID: 2023_09_22_1def59bea80d6e9f67ccg has no ground truth.\n",
            "Paper ID: 2023_09_22_7c5bbe7e076779b790ccg has no ground truth.\n",
            "Paper ID: 2023_09_22_9a007c3865721f379b39g has no ground truth.\n",
            "Paper ID: 2023_09_22_24d7d9ed97e042af9f29g has no ground truth.\n",
            "Paper ID: 2023_09_22_84af4abb781aeead403eg has no ground truth.\n",
            "Paper ID: 2023_09_22_f7714e3a468c91c6f56ag has no ground truth.\n",
            "Paper ID: 2023_09_22_8b46d7b3e561e7f28495g has no ground truth.\n",
            "Paper ID: 2023_09_22_edae82c7fbe0c4062118g has no ground truth.\n",
            "Paper ID: 2023_09_22_2d35eebe2e85cecf4103g has no ground truth.\n",
            "Paper ID: 2023_09_22_6ac063e0fb85cfc10dd0g has no ground truth.\n",
            "Paper ID: 2023_09_22_9a591ebf98377fd0ebe2g has no ground truth.\n",
            "Paper ID: 2023_09_22_cc792b66f9a5779f9798g has no ground truth.\n",
            "Paper ID: 2023_09_22_1b00ed3a142a7a1b2582g has no ground truth.\n",
            "Paper ID: 2023_09_22_a0271d2dc7b0f2506498g has no ground truth.\n",
            "Paper ID: 2023_09_22_87d405f182ae3706ea0cg has no ground truth.\n",
            "DeepSeek-R1-Distill-Qwen-32B\n",
            "DFT\n",
            "0.12193712698502676\n",
            "0.17796610169491525\n",
            "0.09463587260197431\n"
          ]
        }
      ],
      "source": [
        "\n",
        "f1=0.0\n",
        "precision=0\n",
        "recall=0\n",
        "valid_num=0\n",
        "for data in results:\n",
        "    if data['num_ground_truth'] == 0:\n",
        "        print(f\"Paper ID: {data['paper_id']} has no ground truth.\")\n",
        "        continue\n",
        "    f1+=data[\"f1\"]\n",
        "    precision+=data[\"precision\"]\n",
        "    recall+=data[\"recall\"]\n",
        "    valid_num+=1\n",
        "print(INFER_MODEL_NAME)\n",
        "print(TASK_NAME)\n",
        "print(f1/valid_num)\n",
        "print(precision/valid_num)\n",
        "print(recall/valid_num)\n",
        "# Save the results to a JSON file\n",
        "output_file = os.path.join(inference_path, \"..\", f\"{INFER_MODEL_NAME} ave_accuracy.json\")\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump({\"f1\":f1/valid_num,\"precision\":precision/valid_num,\"recall\":recall/valid_num}, f, indent=4, ensure_ascii=False)\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "LpCKZtdqZAgj",
        "k1NbBXyWZD7C",
        "NwlQaQlVmRcy",
        "gHb00g_6mTRS",
        "7zqg5F8Ll9BO",
        "SBaTJhefmM8l"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "d2l",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
